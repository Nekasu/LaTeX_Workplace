\section{Ebaluation Metrics}

Due to the relatively recent development of the style transfer field\citep{02gatys2016image} and the difficulty of objectively evaluating the quality of stylized images, there are currently numerous evaluation standards in the style transfer domain. Researchers often struggle to find widely accepted objective metrics when attempting to evaluate a model's style transfer capability. Therefore, this paper has analyzed some evaluation standards used in style transfer-related literature published over the past three years in conferences such as CVPR, ICCV, ECCV, and AAAI, for reference.

\textbf{Content Loss} and \textbf{Style Loss} are the loss functions used in the first work on neural style transfer\citep{02gatys2016image}, and some scholars still consider these two metrics as standards for image generation. The calculation method for Content Loss is shown in Equation 2, and the calculation method for Style Loss is shown in Equation 5.

\textbf{Time and Memory Usage} are used to measure the efficiency and practicality of an algorithm in style transfer. Time usage reflects the length of time required for an algorithm to complete a task, which is especially important for applications that require quick responses. Memory usage indicates the demand for computational resources when processing images, which is particularly critical for resource-limited devices, such as mobile or embedded systems. Therefore, these two metrics are of significant importance in evaluating and selecting style transfer algorithms that meet specific needs.

The concept of \textbf{Deception Rate} was first introduced by\citep{73sanakoyeu2018style} in 2018, and is described as follows: Train a VGG16 network to distinguish between different artworks from 624 artists in the WikiArt dataset. On this basis, the Deception Rate is defined as:
\begin{equation}
    \begin{aligned}
        \text{Deception Rate} = \frac{F_{cs}^t}{F_{cs}}
    \end{aligned}
\end{equation}
where $F_{cs}$ represents all stylized images, and  $F_{cs}^t$t denotes the number of stylized images that are recognized by VGG16 as artworks created by an artist. Under this definition, Deception Rate represents the proportion of stylized images that are perceived as artist-created rather than computer-generated.

\textbf{FID (Fréchet Inception Distance)} is a commonly used metric for evaluating the output quality of generative models, especially in the field of image generation. This metric was first introduced by Heusel et al.\citep{74heusel2017gans}. FID evaluates the quality of generated images by comparing the distribution of generated images to that of real images in feature space. Specifically, FID calculates the Fréchet distance (also known as Wasserstein-2 distance) between the distribution of generated data and the distribution of real data in the feature space output by a layer of the Inception network. The FID calculation process is as follows:
\begin{enumerate}
    \item  Feature Extraction: Use a specific layer of the Inception-v3 network (typically a layer after pooling) to extract features from both generated images and real images.
    \item Compute Statistics: For the two sets of features (generated image features and real image features), calculate the mean and covariance matrix for each.
    \item Calculate Fréchet Distance: Use the following formula 12 to compute the Fréchet distance between the two multivariate Gaussian distributions:
    \begin{equation}
        \label{FID_equation}
        \text{FID}(x, g) = \Vert\mu_x - \mu_g\Vert^2 + Tr\left((\Sigma_x + \Sigma_g - 2(\Sigma_x\Sigma_g)^{\frac{1}{2}})\right)
    \end{equation}
\end{enumerate}
where $ \mu_x$ and $\Sigma_x $ are the mean and covariance matrix of the real image features, and $\mu_g$ and $\Sigma_g$ are the mean and covariance matrix of the generated image features.

A lower FID value indicates higher quality of generated images, meaning that the distribution of generated images is closer to the distribution of real images. The advantage of FID is that it considers not only the quality of individual images but also the overall diversity of the generated image set. Compared to other evaluation metrics such as Inception Score, FID more accurately reflects the differences perceived by human vision, which is why it has gained widespread use in the field of image generation. However, the FID score also has certain limitations, such as its sensitivity to different datasets and model architectures, and it requires significant computational resources for calculation.

\textbf{Learned Perceptual Image Patch Similarity (LPIPS)} is primarily used to measure the visual similarity between two images and was first introduced by Zhang et al. in 2018\citep{75zhang2018unreasonable}. Unlike traditional pixel-level comparison methods, LPIPS focuses more on perceptual differences between images. It utilizes deep learning models, especially pre-trained neural networks, to simulate the human visual system's perception of image differences. Specifically, the calculation method of LPIPS can be described through the following steps:
\begin{enumerate}
    \item Feature Extraction: For two images $x$ and $y$, use a pre-trained deep neural network (such as AlexNet, VGG, etc.) to extract their features at multiple levels $l$, denoted as $F^l (x)$ and $F^l (y)$.
    \item Compute Normalized Feature Differences: For each spatial location $i$ at each layer, compute the feature difference between $x$ and $y$ at that location, and normalize the features. The difference is calculated using the following formula:
    \begin{equation}
        d_i^l = \frac{1}{N_l} \Vert_i^l(x) - F_i^l(y)|Vert_2^2
    \end{equation}
    where $N_l$ is the number of channels in layer $l$, and $||\cdot||_2$ denotes the L2 norm.
    \item Apply Learned Weights and Aggregate: The feature differences at each layer are weighted by a learned weight $w_l$, and then the weighted differences across all layers and spatial locations are summed to obtain the final LPIPS distance:
    \begin{equation}
        LPIPS(x, y) = \sum_l \sum_i w_l \cdot d_i^l
    \end{equation}
    where $w_l$ is the learned weight for layer $l$ through the training process.
\end{enumerate}

A lower LPIPS score indicates that the two images are more similar on a perceptual level. This metric better simulates the human visual system's perceptual characteristics by considering features from multiple layers of the network and adjusting the importance of features at different layers using learned weights.

\textbf{}{Structural Similarity Index Measure (SSIM)} is a metric used to assess image quality by quantifying the visual similarity between two images. Unlike traditional pixel-based error measures, SSIM considers the structural information of images, better simulating the human visual perception system. The core idea of SSIM is based on the observation that the human visual system is highly adapted to extract structural information from visual scenes. Therefore, SSIM evaluates image quality by considering three comparison dimensions: luminance, contrast, and structure. Specifically, for two images x and y, SSIM is defined as:

\begin{equation}
    SSIM(x, y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}
\end{equation}
where $\mu_x$ and $\mu_y$ are the mean luminances of images $x$ and $y$, $\sigma_x^2$ and $\sigma_y^2$ are the variances of images $x$ and $y$, and $\sigma_{xy}$ is the covariance between images $x$ and $y$. Constants $c_1$ and $c_2$ are small constants added to avoid division by zero, typically defined as $c_1=(k_1 L)^2$ and $c_2=(k_2 L)^2$, where $L$ is the dynamic range of pixel values, and $k_1$ and $k_2$ are constants chosen to be small.

The SSIM value ranges from 0 to 1, where 1 indicates that the two images are identical. SSIM is more accurate and aligns better with human visual perception when evaluating image quality than metrics based solely on pixel differences, such as MSE (Mean Squared Error), as it considers luminance, contrast, and structural aspects.

\textbf{Pixel Distance (PD)} was first proposed by Wang et al.\citep{76wang2020diversified}. This evaluation metric measures diversity by calculating the average distance between sample pairs in both pixel space and deep feature space. Specifically, the average pixel distance between generated image pairs is calculated. For distances in pixel space, the average pixel distance between two images is directly computed across the RGB channels, with the calculation formula as follows:
\begin{equation}
    d_{pixel}(\tilde{x}_1, \tilde{x}_2) = \frac{||\tilde{x}_1 - \tilde{x}_2||_1}{W \times H \times 255 \times 3}
\end{equation}

where $\tilde{x}_1$ and $\tilde{x}_2$ represent the two images for which the pixel distance is to be calculated, and $W$ and $H$ are the width and height of the images. This method quantifies the degree of visual difference between image pairs.

\textbf{User Surveys} are crucial in evaluating style transfer methods because they provide subjective assessments of the style transfer results. While technical metrics can objectively measure certain aspects, the effectiveness of artistic style transfer often relies more on human vision and perception. Through questionnaires, researchers can gather feedback on user satisfaction, visual appeal, and realism of the style transfer results, allowing for a more comprehensive evaluation and improvement of style transfer methods. Based on our knowledge, most neural style transfer papers published after 2019 involved user surveys to demonstrate the superiority of their method in generating stylized images.

Table \ref{table1_Evaluation} in the appendix shows the evaluation metrics used in the papers mentioned in this article.

\begin{table}[t]%% placement specifier
    \centering%% For centre alignment of tabular.
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccccccc}%% Table column specifiers
                \toprule
                Paper & Year & Content\& Sytle Loss & Time & Memory Cost & Deception Rate& FID& LPIPS&SSIM&Content Fidelity&PD&User Study & Others\\
                \midrule
                Gatys et al.\citep{02gatys2016image}&2016 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Li et al.\citep{03li2023frequency} & 2023 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\    
                Huang et al.\citep{04huang2017arbitrary} & 2017 &1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
                Ke et al.\citep{05ke2023neural} & 2023 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
                Johnson et al.\citep{22johnson2016perceptual} & 2016 &0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
                Ulyanov et al.\citep{23ulyanov2016texture} & 2016 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Risser et al.\citep{27risser2017stable} & 2017 &0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Li et al.\citep{28li2017demystifying} & 2017 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Li et al.\citep{33li2016combining} & 2016 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
                Li et al.\citep{35li2016precomputed} & 2016 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
                Zhu et al.\citep{37zhu2017unpaired} & 2017 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
                Men et al.\citep{46Men_2022_CVPR} & 2022 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
                Dumoulin et al.\citep{39dumoulin2016learned} & 2017 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
                Chen et al.\citep{40chen2017stylebank} & 2017 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Jing et al.\citep{41jing2020dynamic} & 2020 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Chen et al.\citep{43chen2016fast} & 2016 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Xu et al.\citep{44xu2021drb} & 2021 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Yang et al.\citep{45yang2022pastiche} & 2022 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                Wu et al.\citep{47wu2023preserving} & 2023 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
                Liu et al.\citep{48liu2021adaattn} & 2021 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
                Deng et al.\citep{49deng2022stytr2} & 2022 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Li et al.\citep{50li2023compact} & 2023 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Zhang et al.\citep{51zhang2024rethink} & 2024 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
                Wang et al.\citep{52wang2023interactive} & 2023 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Zhang et al.\citep{53zhang2023edge} & 2023 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                Hong et al.\citep{54hong2023aespa} & 2023 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
                Zhu et al.\citep{55zhu2023all} & 2023 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
                Kwon et al.\citep{57kwon2022clipstyler} & 2022 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                Hamazaspyan et al.\citep{59hamazaspyan2023diffusion} & 2023 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\
                Zhang et al.\citep{60zhang2024artbank} & 2023 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
                Rombach et al.\citep{61rombach2022high} & 2022 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Zhang et al.\citep{62zhang2023inversion} & 2023 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                AHN et al.\citep{63ahn2024dreamstyler} & 2024 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Wang et al.\citep{64wang2023stylediffusion} & 2023 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
                Lu et al.\citep{65lu2023specialist} & 2023 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Chung et al.\citep{66chung2024style} & 2024 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
                Deng et al.\citep{67deng2024z} & 2024 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
                Chen et al.\citep{70chen2019drop} & 2019 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
                Kwon et al.\citep{71kwon2024aesfa} & 2023 & 1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 \\
                Wang et al.\citep{72wang2023microast} & 2023 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 \\
                Lin et al.\citep{78lin2023adacm} & 2023 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
                Chen et al.\citep{80cheng2023user} & 2023 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\
                \bottomrule
            \end{tabular}
            %% Use \caption command for table caption and label.
    }
    \caption{Evaluation Metrics Used in Papers. The first column of the table presents the papers along with their respective authors, while the second column indicates the year of publication. The remaining columns, starting from the third, represent the objective evaluation metrics used for style transfer. In the evaluation metrics section, a value of 1 indicates that the paper applied the corresponding metric for evaluation, whereas a value of 0 indicates that the metric was not used.}\label{table1_Evaluation}
\end{table}

