%File: formatting-instructions-latex-2026.tex
%release 2026.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{amssymb}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{AAAI Press Formatting Instructions \\for Authors Using \LaTeX{} --- A Guide}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Anonymous Submission
}
% \affiliations{
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name\textsuperscript{\rm 1,\rm 2},
%     Second Author Name\textsuperscript{\rm 2},
%     Third Author Name\textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Affiliation 1\\
%     \textsuperscript{\rm 2}Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
% You must keep this block between (not within) the abstract and the main body of the paper.
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}

Neural style transfer (NST) reimagines visual content by synthesizing the semantics of one image with the artistic identity of another using deep neural networks. 
Since the seminal work of Gatys et al.~\cite{gatys2016image}, which framed style transfer as an optimization problem using the Gram matrix of VGG features, the field has advanced significantly toward real-time stylization~\cite{huang2017arbitrary}, high-fidelity generation~\cite{kwon2024aesfa}, and multimodal controllability~\cite{ahn2024dreamstyler}.

However, a critical limitation underlies almost all existing NST models: the assumption that input images are fully opaque. This design paradigm excludes the alpha channel—a key component in digital imaging that encodes per-pixel transparency and is widely used in UI composition, image compositing, and layered artistic workflows. As a result, standard NST methods ignore transparency semantics, which often leads to undesirable stylization artifacts such as edge bleeding, occluded detail transfer, and inconsistency in downstream composition.

Unlike RGB channels that define visual appearance, the alpha channel governs perceptual visibility. In the context of style transfer, alpha values modulate stylistic salience—pixels with high alpha demand stronger stylization fidelity, while those with low or zero alpha suppress stylistic rendering. This transparency signal encodes important spatial semantics that are entirely discarded in RGB-only NST pipelines.

To address this gap, we introduce \textbf{alpha-aware neural style transfer}—a novel NST paradigm that models transparency semantics throughout the stylization process. Our end-to-end framework accepts RGBA inputs, applies alpha-guided feature computation, and outputs stylized RGBA images that faithfully preserve both artistic identity and transparency structure. To our knowledge, this is the first NST method to treat soft alpha information as a first-class modeling component.

While our study focuses on the alpha channel as a prominent transparency cue, we view our design as a first step toward modeling more general per-pixel semantics such as depth, surface normals, or material attributes. We leave these extensions for future work.

Our contributions are summarized as follows:
\begin{itemize}
    \item {Problem formulation}: We define alpha-aware neural style transfer as a new task setting that extends traditional RGB-only NST to RGBA-space modeling with explicit transparency semantics.
    \item {Soft Partial Convolution}: We propose a soft partial convolution module that generalizes conventional partial convolutions to handle continuous-valued alpha masks, enabling effective feature propagation under partial visibility.
    \item {Alpha-guided perceptual loss}: We design an alpha-aware perceptual loss that modulates stylization supervision based on transparency, encouraging semantic consistency between visibility and stylized output.
    \item {AlphaStyle Dataset}: We construct AlphaStyle, the first large-scale RGBA dataset for NST, derived from WikiArt and MSCOCO using Segment Anything~\cite{kirillov2023segment} to extract irregular masks. The dataset contains over 20,000 high-quality RGBA image pairs in lossless PNG format.
\end{itemize}

\section{Realted Works}

% Main goal of this chapter is showing the absence of alpha channel in NST. How to illustrate this phenomenon? 

% First of all, we can introduce the prior works, such as AdaIN. By showing the achievements of these important works, the importance of our work can bee seen--cover the absence of alpha-channels.

% Additionally, we can emphesize the imoprtance of 

% 诚然, Related Works 一节是在介绍前人工作, 但介绍前人工作的同时, 也应该突出我们工作的重要性. 由于这一节以介绍其他工作为主, 所以在选择突出本文工作重要性方面, 我们主要应用的是“对比”, 即对比其他工作与该工作之间的差异. 

% 但对比也应该是有技巧的, 我们可以列举从多个角度发展风格迁移的前人文章(如风格特征提取、超高分辨率图像风格迁移、人机交互的风格迁移等不同研究方向的前人风格迁移文章), 从而突出表明, 在多个风格迁移发展角度, 都没有人关注 α 通道这一方面的信息, 是严重的失误.

% 我们还可以从发展的角度看, 即从纵向角度观察风格迁移发展. 列举风格迁移发展期间的众多关键性工作. 从时间的角度来说明被忽略的 α 通道. 结合上一段的思路, 即是结合了横向与纵向的发展角度, 整体考虑的差异性.

% 我们还可以从实现的方法角度说明这个问题. 即当前存在的方法考虑了空间, 而未考虑能见性. 

\subsection{Neural Style Transfer}

Neural Style Transfer (NST) aims to synthesize an image that preserves the semantic content of a source image while reflecting the artistic style of another. The field was initiated by Gatys et al.~\cite{gatys2016image}, who formulated NST as an optimization problem by minimizing a style-content loss defined on the Gram matrices of deep feature maps extracted by a pretrained VGG network~\cite{25simonyan2014very}. This seminal work demonstrated the capability of convolutional features to separate and recombine style and content.

However, optimization-based methods~\cite{li2016combining, berger2016incorporating, risser2017stable} are computationally expensive, requiring a forward and backward pass for each output image. To address efficiency, feed-forward approaches were proposed, training a generative model to approximate the style transformation. Early works~\cite{johnson2016perceptual, ulyanov2016texture} trained one model per style, while later extensions~\cite{dumoulin2016learned, chen2017stylebank} enabled multiple styles per model. More recent methods~\cite{huang2017arbitrary, xu2021drb, kwon2024aesfa} support arbitrary style transfer via feature statistic alignment or adaptive modulation.

Despite substantial progress in efficiency, fidelity, and control, all aforementioned methods operate under a common assumption: input images are fully opaque and represented solely in RGB space. This assumption simplifies modeling but ignores transparency semantics crucial to layered visual content, rendering these methods ineffective when dealing with partial visibility or alpha-masked regions.
% Neural Style Transfer(NST) is an image generation task which absorb the content of one image and style of another.
% With researches\cite{24mordvintsev2015} on the feature extraction ability of VGG\cite{25simonyan2014very}, Gatys et al.\cite{gatys2016image} first used the certain layer of VGG while encoding a style image and a content image to construct a loss function for optimizing a noisy image, finally got a stylized image. After that, style transfer developed into a new stage. To generate better image, Markov Random Field\cite{li2016combining} and other methods\cite{berger2016incorporating,risser2017stable} are proposed. Forementioned works, which are called pixel-iteative NST all do image optimization, which is time-costing.
% To improve the efficiency, model-iterative NST are proposed.This kind of NST develop from per-model-per-style\cite{johnson2016perceptual,ulyanov2016texture} to per-model-multi-style\cite{dumoulin2016learned,chen2017stylebank}, fianlly to per-model-arbitrary-style\cite{huang2017arbitrary,xu2021drb,kwon2024aesfa,zhu2023all}.
% Despite the massive progress in NST, above works all assume fully opaque content and are not applicable to stylization under partial visibility.

\subsection{Transparency Modeling in Vision Tasks}

While transparency is largely ignored in current neural style transfer methods, it plays a significant role in several other vision tasks. For instance, in image matting~\cite{yao2024matte}, the alpha channel is a central component used to separate foreground from background, guiding the generation of high-fidelity composite images. Recent works such as Matte Anything~\cite{yao2024matte} leverage large vision models to predict continuous alpha mattes, demonstrating that soft transparency cues provide essential structure for downstream editing.

Beyond traditional compositing, transparency has also been used as a medium for adversarial attack design. Xia et al.~\cite{xia2025alphadog} introduced AlphaDog, an adversarial example framework that manipulates the alpha channel to fool Large Language Models (LLMs) equipped with vision encoders. They show that many vision-language models rely solely on RGB content, neglecting alpha information. As a result, alpha-perturbed images are perceptually unchanged to humans but lead to erroneous model predictions—highlighting a critical semantic gap between human and model perception.

These applications affirm that the alpha channel encodes nontrivial semantics relevant to content structure and visibility. Yet, to the best of our knowledge, such transparency cues have never been explicitly modeled in neural style transfer. Our work fills this gap by incorporating soft alpha information into both feature learning and stylization loss functions.
% \subsection{Transparency Modeling in Vision Tasks}

% Transparency is utilized in some vision tasks.
% As a core concept of image matting, transparency is used to distinguish main body and background~\cite{yao2024matte}. 
% Transparency Modeling is also used to cheat some LLMs(Large Language Models), which neerly pay no attention to alpha channel, in the field of adversarial example~\cite{xia2025alphadog}. Xia et al. found that LLMs only analyze RGB channels of RGBA input images, while humans are influenced by all channels. As a result, the same image will be recognized as two different ones by LLMs and humans, so that the LLMs can be cheated.



% \subsection{Masked Feature Propagation}

% Handling partially visible or spatially masked regions is a longstanding challenge in image processing. Partial Convolution, introduced by Liu et al.~\cite{liu2018image}, was originally designed for image inpainting under binary masks. It conditionally normalizes convolution responses by considering only the unmasked (valid) pixels, enabling reliable feature propagation from structurally intact regions. Gated Convolution~\cite{yu2019free} extended this idea by learning soft gating masks that adaptively modulate feature flow based on input content.

% Subsequent works have further improved soft or probabilistic masking mechanisms. For instance, Multi-feature Co-learning introduces soft-gating dual feature fusion for structure and texture blending~\cite{lin2022multi}, and Continuously Masked Transformer~\cite{ko2023continuously} applies continuous masks for guiding masked self-attention. Despite their effectiveness in image restoration, these methods remain focused on spatial reconstruction and structural completion––they do not model stylistic transformation under artistic constraints.

% In the style transfer literature, soft masks have occasionally been integrated to guide selective stylization. For example, Zhao et al.~\cite{zhao2020automatic} use soft semantic masks from image segmentation to preserve object boundaries during texture mapping. Similarly, approaches like Class-Based Styling (CBS)~\cite{kurzman2019class} and SEAN~\cite{zhu2019sean} apply segmentation-derived masks to control region-specific styling. However, these methods use masks as category priors, not as transparency signals, and their loss functions are not designed to respect pixel-level visibility.

% In contrast, our approach is the first to leverage the alpha channel itself as a soft mask carrying continuous semantics of visibility. We introduce soft partial convolution, which integrates continuous alpha values into feature computation, and design alpha-guided perceptual losses that modulate stylization according to visibility. This makes our method uniquely transparency-aware in the context of neural style transfer.

\subsection{Visibility-Aware Feature Propagation}

Stylization under partial visibility presents unique challenges: not all pixels contribute equally to the perceptual structure or aesthetic of an image. In traditional computer vision tasks such as inpainting, techniques like partial convolution~\cite{liu2018image} and gated convolution~\cite{yu2019free} were introduced to address irregular missing regions by modulating feature propagation with spatial masks. These methods condition the convolution operation on binary or learned soft masks to restrict computation to valid areas, enabling reconstruction in structurally incomplete inputs.

Some recent approaches also extend the use of (soft) masks to guide feature modulation. For instance, segmentation-aware stylization~\cite{zhao2020automatic,yu2024foreground, ko2023continuously} employs semantic (soft) masks to preserve object-level consistency across style domains. These methods treat masks as external priors—typically reflecting semantic class identity or object boundaries—but do not address the notion of visibility as a first-class semantic cue.

Our approach differs in both motivation and formulation. Instead of treating masks as auxiliary spatial signals, we regard the alpha channel as a native representation of pixel-level visibility semantics. It directly encodes how strongly each pixel should participate in stylization, offering a more principled foundation for visibility-aware feature routing. To realize this, we implement a \textit{soft alpha-guided feature propagation} mechanism, where alpha values modulate both intermediate features and perceptual losses. This extends prior masked-feature methods beyond spatial reconstruction, enabling stylization to be conditioned on natural transparency cues embedded in RGBA data.

To our knowledge, this is the first attempt to integrate visibility-aware guidance—derived from alpha transparency—into the stylization process in an end-to-end learnable framework.

\section{Method}

\subsection{Problem Formulation: Alpha-aware Neural Style Transfer}

We define \textbf{Alpha-aware Neural Style Transfer (A-NST)} as an extension of traditional neural style transfer from the RGB domain to the RGBA space, where the alpha channel provides pixel-level transparency signals. In contrast to conventional NST, which assumes full visibility across all pixels, A-NST explicitly incorporates \textit{visibility semantics}—i.e., where and to what extent style should be rendered—guided by the alpha channel.

Formally, let $I_c \in \mathbb{R}^{H \times W \times 4}$ and $I_s \in \mathbb{R}^{H \times W \times 4}$ be the content and style images in RGBA format, respectively. The objective is to synthesize a stylized output $I_t \in \mathbb{R}^{H \times W \times 4}$ such that:

\begin{itemize}
    \item The RGB channels of $I_t$ preserve the structural layout of $I_c$ while reflecting the style characteristics of $I_s$;
    \item The alpha channel of $I_t$ conveys meaningful transparency, inherited from $I_c$ and/or $I_s$ depending on task constraints.
\end{itemize}

This problem formulation introduces a new modeling dimension in NST: transparency-aware processing. To this end, we propose a framework that integrates soft alpha-guided feature propagation and visibility-weighted loss computation, ensuring style synthesis aligns with the perceptual transparency structure of the input.

\bibliography{aaai2026}

\end{document}
