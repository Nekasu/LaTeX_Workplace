%File: formatting-instructions-latex-2026.tex
%release 2026.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{amssymb}
\usepackage{amsmath}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Transparency-Aware Style Transfer via Soft Alpha-Guided Feature Propagation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Anonymous Submission
}
% \affiliations{
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name\textsuperscript{\rm 1,\rm 2},
%     Second Author Name\textsuperscript{\rm 2},
%     Third Author Name\textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1}Affiliation 1\\
%     \textsuperscript{\rm 2}Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Neural style transfer (NST) aims to synthesize images that retain the structural content of one image while adopting the visual style of another. While recent methods have advanced stylization quality and speed, they typically assume fully opaque inputs and disregard the alpha channel commonly present in RGBA images. However, pixel-wise transparency is essential in many practical settings, including compositional graphics, UI layering, and even adversarial attacks where hidden content is encoded in alpha channels.
We introduce the task of alpha-aware neural style transfer (A-NST), which generalizes conventional NST to handle partially visible inputs. To address this challenge, we propose a novel framework called Soft Alpha-Guided Feature Propagation (SAFP), where the alpha channel serves as a continuous visibility prior to guide feature modulation throughout the stylization process. 
To evaluate A-NST, we construct AlphaStyle, a new dataset comprising RGBA-style pairs with varying transparency levels, derived from MSCOCO and WikiArt via segmentation and compositing. Experiments demonstrate that our method significantly outperforms RGB-only baselines in preserving structural fidelity and transparency alignment. To our knowledge, this is the first formal study to frame neural style transfer in the RGBA domain.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
% You must keep this block between (not within) the abstract and the main body of the paper.
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}

Neural style transfer (NST) reimagines visual content by synthesizing the semantics of one image with the artistic identity of another using deep neural networks. 
Since the seminal work of Gatys et al.~\cite{gatys2016image}, which framed style transfer as an optimization problem using the Gram matrix of VGG features, the field has advanced significantly toward real-time stylization~\cite{huang2017arbitrary}, high-fidelity generation~\cite{kwon2024aesfa}, and multimodal controllability~\cite{ahn2024dreamstyler}.

However, a critical limitation persists across almost all existing neural style transfer (NST) models: the implicit assumption that input images are fully opaque. This design paradigm effectively excludes the alpha channel—an integral component of digital imagery that encodes per-pixel transparency. The alpha channel is central to layered graphics, UI composition, and image compositing workflows, where transparency governs how visual elements interact across spatial hierarchies. By ignoring this dimension, conventional NST models fail to account for transparency semantics, often resulting in stylization artifacts such as edge bleeding, style leakage into transparent regions, and compositional inconsistencies during downstream integration.

Unlike RGB channels that define visual appearance, the alpha channel governs perceptual visibility. In the context of style transfer, alpha values modulate stylistic salience—pixels with high alpha demand stronger stylization fidelity, while those with low or zero alpha suppress stylistic rendering. This transparency signal encodes important spatial semantics that are entirely discarded in RGB-only NST pipelines.

To address this gap, we introduce \textbf{alpha-aware neural style transfer}—a novel NST paradigm that models transparency semantics throughout the stylization process. Our end-to-end framework accepts RGBA inputs, applies alpha-guided feature computation, and outputs stylized RGBA images that faithfully preserve both artistic identity and transparency structure. To our knowledge, this is the first NST method to treat soft alpha information as a first-class modeling component.

While our study focuses on the alpha channel as a prominent transparency cue, we view our design as a first step toward modeling more general per-pixel semantics such as depth, surface normals, or material attributes. We leave these extensions for future work.

Our contributions are summarized as follows:
\begin{itemize}
    \item {Problem formulation}: We define alpha-aware neural style transfer as a new task setting that extends traditional RGB-only NST to RGBA-space modeling with explicit transparency semantics.
    % \item {Soft Partial Convolution}: We propose a soft partial convolution module that generalizes conventional partial convolutions to handle continuous-valued alpha masks, enabling effective feature propagation under partial visibility.
    \item {Soft Alpha-Guided Feature Propagation (SAFP)}: We introduce a transparency-aware feature routing mechanism that integrates the alpha channel as a continuous visibility prior throughout the network. 
    \item {Alpha-guided perceptual loss}: We design an alpha-aware perceptual loss that modulates stylization supervision based on transparency, encouraging semantic consistency between visibility and stylized output.
    \item {AlphaStyle Dataset}: We construct AlphaStyle, the first large-scale RGBA dataset for NST, derived from WikiArt and MSCOCO using Segment Anything~\cite{kirillov2023segment} to extract irregular masks. The dataset contains over 20,000 high-quality RGBA image pairs in lossless PNG format.
\end{itemize}

\section{Realted Works}

% Main goal of this chapter is showing the absence of alpha channel in NST. How to illustrate this phenomenon? 

% First of all, we can introduce the prior works, such as AdaIN. By showing the achievements of these important works, the importance of our work can bee seen--cover the absence of alpha-channels.

% Additionally, we can emphesize the imoprtance of 

% 诚然, Related Works 一节是在介绍前人工作, 但介绍前人工作的同时, 也应该突出我们工作的重要性. 由于这一节以介绍其他工作为主, 所以在选择突出本文工作重要性方面, 我们主要应用的是“对比”, 即对比其他工作与该工作之间的差异. 

% 但对比也应该是有技巧的, 我们可以列举从多个角度发展风格迁移的前人文章(如风格特征提取、超高分辨率图像风格迁移、人机交互的风格迁移等不同研究方向的前人风格迁移文章), 从而突出表明, 在多个风格迁移发展角度, 都没有人关注 α 通道这一方面的信息, 是严重的失误.

% 我们还可以从发展的角度看, 即从纵向角度观察风格迁移发展. 列举风格迁移发展期间的众多关键性工作. 从时间的角度来说明被忽略的 α 通道. 结合上一段的思路, 即是结合了横向与纵向的发展角度, 整体考虑的差异性.

% 我们还可以从实现的方法角度说明这个问题. 即当前存在的方法考虑了空间, 而未考虑能见性. 

\subsection{Neural Style Transfer}

Neural Style Transfer (NST) aims to synthesize an image that preserves the semantic content of a source image while reflecting the artistic style of another. The field was initiated by Gatys et al.~\cite{gatys2016image}, who formulated NST as an optimization problem by minimizing a style-content loss defined on the Gram matrices of deep feature maps extracted by a pretrained VGG network~\cite{25simonyan2014very}. This seminal work demonstrated the capability of convolutional features to separate and recombine style and content.

However, optimization-based methods~\cite{li2016combining, berger2016incorporating, risser2017stable} are computationally expensive, requiring a forward and backward pass for each output image. To address efficiency, feed-forward approaches were proposed, training a generative model to approximate the style transformation. Early works~\cite{johnson2016perceptual, ulyanov2016texture} trained one model per style, while later extensions~\cite{dumoulin2016learned, chen2017stylebank} enabled multiple styles per model. More recent methods~\cite{huang2017arbitrary, xu2021drb, kwon2024aesfa} support arbitrary style transfer via feature statistic alignment or adaptive modulation.

Despite substantial progress in efficiency, fidelity, and control, all aforementioned methods operate under a common assumption: input images are fully opaque and represented solely in RGB space. This assumption simplifies modeling but ignores transparency semantics crucial to layered visual content, rendering these methods ineffective when dealing with partial visibility or alpha-masked regions.
% Neural Style Transfer(NST) is an image generation task which absorb the content of one image and style of another.
% With researches\cite{24mordvintsev2015} on the feature extraction ability of VGG\cite{25simonyan2014very}, Gatys et al.\cite{gatys2016image} first used the certain layer of VGG while encoding a style image and a content image to construct a loss function for optimizing a noisy image, finally got a stylized image. After that, style transfer developed into a new stage. To generate better image, Markov Random Field\cite{li2016combining} and other methods\cite{berger2016incorporating,risser2017stable} are proposed. Forementioned works, which are called pixel-iteative NST all do image optimization, which is time-costing.
% To improve the efficiency, model-iterative NST are proposed.This kind of NST develop from per-model-per-style\cite{johnson2016perceptual,ulyanov2016texture} to per-model-multi-style\cite{dumoulin2016learned,chen2017stylebank}, fianlly to per-model-arbitrary-style\cite{huang2017arbitrary,xu2021drb,kwon2024aesfa,zhu2023all}.
% Despite the massive progress in NST, above works all assume fully opaque content and are not applicable to stylization under partial visibility.

\subsection{Transparency Modeling in Vision Tasks}

While transparency is largely ignored in current neural style transfer methods, it plays a significant role in several other vision tasks. For instance, in image matting~\cite{yao2024matte}, the alpha channel is a central component used to separate foreground from background, guiding the generation of high-fidelity composite images. Recent works such as Matte Anything~\cite{yao2024matte} leverage large vision models to predict continuous alpha mattes, demonstrating that soft transparency cues provide essential structure for downstream editing.

Beyond traditional compositing, transparency has also been used as a medium for adversarial attack design. Xia et al.~\cite{xia2025alphadog} introduced AlphaDog, an adversarial example framework that manipulates the alpha channel to fool Large Language Models (LLMs) equipped with vision encoders. They show that many vision-language models rely solely on RGB content, neglecting alpha information. As a result, alpha-perturbed images are perceptually unchanged to humans but lead to erroneous model predictions—highlighting a critical semantic gap between human and model perception.

These applications affirm that the alpha channel encodes nontrivial semantics relevant to content structure and visibility. Yet, to the best of our knowledge, such transparency cues have never been explicitly modeled in neural style transfer. Our work fills this gap by incorporating soft alpha information into both feature learning and stylization loss functions.
% \subsection{Transparency Modeling in Vision Tasks}

% Transparency is utilized in some vision tasks.
% As a core concept of image matting, transparency is used to distinguish main body and background~\cite{yao2024matte}. 
% Transparency Modeling is also used to cheat some LLMs(Large Language Models), which neerly pay no attention to alpha channel, in the field of adversarial example~\cite{xia2025alphadog}. Xia et al. found that LLMs only analyze RGB channels of RGBA input images, while humans are influenced by all channels. As a result, the same image will be recognized as two different ones by LLMs and humans, so that the LLMs can be cheated.



% \subsection{Masked Feature Propagation}

% Handling partially visible or spatially masked regions is a longstanding challenge in image processing. Partial Convolution, introduced by Liu et al.~\cite{liu2018image}, was originally designed for image inpainting under binary masks. It conditionally normalizes convolution responses by considering only the unmasked (valid) pixels, enabling reliable feature propagation from structurally intact regions. Gated Convolution~\cite{yu2019free} extended this idea by learning soft gating masks that adaptively modulate feature flow based on input content.

% Subsequent works have further improved soft or probabilistic masking mechanisms. For instance, Multi-feature Co-learning introduces soft-gating dual feature fusion for structure and texture blending~\cite{lin2022multi}, and Continuously Masked Transformer~\cite{ko2023continuously} applies continuous masks for guiding masked self-attention. Despite their effectiveness in image restoration, these methods remain focused on spatial reconstruction and structural completion––they do not model stylistic transformation under artistic constraints.

% In the style transfer literature, soft masks have occasionally been integrated to guide selective stylization. For example, Zhao et al.~\cite{zhao2020automatic} use soft semantic masks from image segmentation to preserve object boundaries during texture mapping. Similarly, approaches like Class-Based Styling (CBS)~\cite{kurzman2019class} and SEAN~\cite{zhu2019sean} apply segmentation-derived masks to control region-specific styling. However, these methods use masks as category priors, not as transparency signals, and their loss functions are not designed to respect pixel-level visibility.

% In contrast, our approach is the first to leverage the alpha channel itself as a soft mask carrying continuous semantics of visibility. We introduce soft partial convolution, which integrates continuous alpha values into feature computation, and design alpha-guided perceptual losses that modulate stylization according to visibility. This makes our method uniquely transparency-aware in the context of neural style transfer.

\subsection{Visibility-Aware Feature Propagation}

Stylization under partial visibility presents unique challenges: not all pixels contribute equally to the perceptual structure or aesthetic of an image. In traditional computer vision tasks such as inpainting, techniques like partial convolution~\cite{liu2018image} and gated convolution~\cite{yu2019free} were introduced to address irregular missing regions by modulating feature propagation with spatial masks. These methods condition the convolution operation on binary or learned soft masks to restrict computation to valid areas, enabling reconstruction in structurally incomplete inputs.

Some recent approaches also extend the use of (soft) masks to guide feature modulation. For instance, segmentation-aware stylization~\cite{zhao2020automatic,yu2024foreground, ko2023continuously} employs semantic (soft) masks to preserve object-level consistency across style domains. These methods treat masks as external priors—typically reflecting semantic class identity or object boundaries—but do not address the notion of visibility as a first-class semantic cue.

Our approach differs in both motivation and formulation. Instead of treating masks as auxiliary spatial signals, we regard the alpha channel as a native representation of pixel-level visibility semantics. It directly encodes how strongly each pixel should participate in stylization, offering a more principled foundation for visibility-aware feature routing. To realize this, we implement a \textit{soft alpha-guided feature propagation} mechanism, where alpha values modulate both intermediate features and perceptual losses. This extends prior masked-feature methods beyond spatial reconstruction, enabling stylization to be conditioned on natural transparency cues embedded in RGBA data.

To our knowledge, this is the first attempt to integrate visibility-aware guidance—derived from alpha transparency—into the stylization process in an end-to-end learnable framework.

\section{Method}

\subsection{Problem Formulation: Alpha-aware Neural Style Transfer}

We define \textbf{Alpha-aware Neural Style Transfer (A-NST)} as an extension of conventional neural style transfer from the RGB space to the RGBA domain, where the alpha channel encodes per-pixel transparency. Unlike standard NST, which assumes full visibility across all pixels, A-NST explicitly models \textit{visibility semantics}—that is, the spatial extent and intensity of stylization—guided by the input alpha map.

Formally, let $I_c \in \mathbb{R}^{H \times W \times 4}$ and $I_s \in \mathbb{R}^{H \times W \times 4}$ denote the content and style images in RGBA format, respectively. The goal is to synthesize a stylized output $I_t \in \mathbb{R}^{H \times W \times 4}$ such that:
\begin{itemize}
    \item The RGB channels of $I_t$ preserve the structural content of $I_c$ while reflecting the style patterns of $I_s$;
    
    \item The alpha channel of $I_t$ conveys semantically meaningful transparency, consistent with $I_c$ and/or $I_s$ depending on the stylization objective.
\end{itemize}

This problem formulation introduces a new dimension to neural stylization: transparency-aware modeling. It demands the network to account not only for appearance transformation, but also for selective feature propagation conditioned on pixel-wise visibility. To address this, we propose an end-to-end framework that incorporates soft alpha-guided feature routing and transparency-aware loss functions to produce stylized RGBA outputs that preserve both artistic identity and alpha structure.

% \subsection{Problem Formulation: Alpha-aware Neural Style Transfer}

% We define \textbf{Alpha-aware Neural Style Transfer (A-NST)} as an extension of traditional neural style transfer from the RGB domain to the RGBA space, where the alpha channel provides pixel-level transparency signals. In contrast to conventional NST, which assumes full visibility across all pixels, A-NST explicitly incorporates \textit{visibility semantics}—i.e., where and to what extent style should be rendered—guided by the alpha channel.

% Formally, let $I_c \in \mathbb{R}^{H \times W \times 4}$ and $I_s \in \mathbb{R}^{H \times W \times 4}$ be the content and style images in RGBA format, respectively. The objective is to synthesize a stylized output $I_t \in \mathbb{R}^{H \times W \times 4}$ such that:

% \begin{itemize}
%     \item The RGB channels of $I_t$ preserve the structural layout of $I_c$ while reflecting the style characteristics of $I_s$;

%     \item The alpha channel of $I_t$ conveys meaningful transparency, inherited from $I_c$ and/or $I_s$ depending on task constraints.
% \end{itemize}

% This problem formulation introduces a new modeling dimension in NST: transparency-aware processing. To this end, we propose a framework that integrates soft alpha-guided feature propagation and visibility-weighted loss computation, ensuring style synthesis aligns with the perceptual transparency structure of the input.

\subsection{Soft Alpha-Guided Feature Propagation}

% 2025-07-23 22:31 周三 版本
We propose \textbf{Soft Alpha-Guided Feature Propagation (SAFP)}, a transparency-aware mechanism that integrates pixel-wise visibility semantics into the stylization process. Instead of treating the alpha channel as an auxiliary mask applied post-hoc, we embed it directly into the convolutional backbone, allowing alpha transparency to dynamically modulate feature propagation throughout the network.

Given an RGBA input image $I \in \mathbb{R}^{H \times W \times 4}$, we decompose it into an RGB tensor $I_{RGB} \in \mathbb{R}^{H \times W \times 3}$ and a normalized alpha map $M_\alpha \in [0, 1]^{H \times W}$. The alpha map is interpreted as a soft visibility prior: higher values indicate greater relevance in stylization, while lower values correspond to transparent or visually suppressed regions.

To incorporate $M_\alpha$ into the feature extraction pipeline, we introduce \emph{Soft Partial Convolution (SoftPConv)}, an extension of Partial Convolution~\cite{liu2018image} to continuous masks. The output feature Y at pixel $(i, j)$ is computed as:

\begin{equation}
Y_{i,j} = 
\begin{cases}
\displaystyle
\frac{
\sum\limits_{(u,v) \in \Omega_{i,j}} W_{u,v} \cdot I_{RGB}^{(u, v)} \cdot M_\alpha^{(u, v)}
}{
\sum\limits_{(u,v) \in \Omega_{i,j}} M_\alpha^{(u, v)}
} + b, & \text{if } \sum M_\alpha > 0 \\
0, & \text{otherwise}
\end{cases}
\label{eq:softpconv}
\end{equation}

Here, $\Omega_{i,j}$ denotes the local kernel window centered at pixel $(i,j)$, $W$ is the convolution kernel, and $b$ is the bias term.

Following~\cite{liu2018image}, we update the alpha map after each layer to reflect the effective visibility at the next stage:

\begin{equation}
M_\alpha'(i,j) = \frac{1}{|\Omega|} \sum\limits_{(u,v) \in \Omega_{i,j}} M_\alpha(u,v)
\label{eq:mask_update}
\end{equation}

We apply SoftPConv throughout the encoder by replacing all vanilla convolutions. This ensures that visibility-aware feature propagation is maintained at every spatial scale, enabling the network to stylize partially visible regions while respecting transparency boundaries and semantic layering. 

Importantly, SoftPConv is implemented as a drop-in replacement for conventional convolutional layers and does not require architectural re-design. As such, it can be seamlessly integrated into existing convolutional architectures, making it a lightweight and modular enhancement for transparency-aware vision tasks.
% 2025-07-23 22:23 周三 版本
% We propose \textbf{Soft Alpha-Guided Feature Propagation (SAFP)}, a framework-level mechanism designed to explicitly integrate pixel-level visibility information encoded in the alpha channel into the stylization process. Unlike conventional neural style transfer approaches that uniformly propagate features without visibility constraints, our method leverages alpha transparency to dynamically modulate feature flow.

% Specifically, given RGBA inputs, we first separate the alpha channel from RGB components. The alpha channel is normalized and interpreted as a spatially continuous visibility mask, directly informing feature extraction and stylization stages. To effectively implement this alpha-guided propagation, we introduce a modified convolutional operation—\textit{Soft Partial Convolution (SoftPConv)}—as the core computational unit. SoftPConv, which is based on Partial Convolution\cite{liu2022partial}, conditions convolutional feature extraction on alpha values, ensuring that transparent regions minimally influence feature activations.
% Supposing that RGBA inputs $I$ have been separate into a normalized alpha channel, also called continuous visible mask $M_{\alpha}$, and RGB parts $I_{RGB}$, the SoftPConv works as the following formula:

% \begin{equation}
%     \begin{aligned}
%         \label{equation-soft_partial_convolution}
%         Y_{i,j} = \begin{cases}
%             \frac{\sum_{k,l} W \cdot I_{RGB}^{(i+k, j+l)} \cdot M_{\alpha}^{(i+k, j+l)}}{\sum M_{\alpha}^{(i+k,j+l)}} & +b,\\
%             &\mathrm{if} \sum_{k,l} M_{\alpha}^{(i+k, j+l)} > 0\\
%             0, & \mathrm{otherwise},
%         \end{cases}
%     \end{aligned}
% \end{equation}
% where $(i,j)$ represents the current pixel, $W$ means a vanilla convolution operator, $k,l$ means the amount of cheapness relative to the current pixel.

% Similar to Partial Convolution, we sample local averages to update the continuous visibe mask $M'_{\alpha}$:

% \begin{equation}
%     M_{\alpha}^{'(i,j)} = \frac{1}{K^2}\sum_{k,l}M_{\alpha}^{(i+k,.j+l)}
% \end{equation}

% Throughout the encoder-decoder pipeline—adapted from the recent state-of-the-art AesFA architecture~\cite{kwon2024aesfa}—we systematically replace all vanilla convolutions with SoftPConv, thus maintaining a consistent visibility-aware feature representation. This allows alpha-derived visibility semantics to be preserved across downsampling and upsampling stages, guiding both local texture adaptation and global structural alignment.

% Figure~X illustrates the detailed alpha-guided feature propagation process within our pipeline.

% 我需要修改这一节. method部分应按如下方式展开: 1. 任务设定, 什么是A-NST(已完成); 2. α引导的特征传递. 解释该名词, 并介绍本文的实现方法: soft partial convolution; 3. 我的文章结构很像gated covolution一样, 参考gated文章中 method 第三小节怎么写的, 来确定我的文章如何展开.

%  2025-07-24 23:05 周四
\subsection{Alpha-Aware Network Architecture}

SoftPConv layers, introduced in the previous section as the implementation of SAFP, replace all vanilla convolutions in the encoder to enable transparency-aware stylization.To validate its effectiveness, we integrate SoftPConv into a recent state-of-the-art architecture for arbitrary style transfer: \textit{AesFA}~\cite{kwon2024aesfa}.

Our design goal is to extend style transfer models (here is AesFA) to operate in the RGBA domain while preserving its expressive capacity and training stability. We therefore adopt a minimally invasive strategy: only convolutional components are modified to support alpha guidance, while higher-level design such as feature adaptation remains unchanged.

\noindent\textbf{Input and Output.}
The model receives two RGBA images as input: a content image 
$I_c \in \mathbb{R}^{H \times W \times 4}$ and a style image $I_s \in \mathbb{R}^{H \times W \times 4}$. The output is a stylized image $I_t \in \mathbb{R}^{H \times W \times 4}$. We decompose each input into RGB and alpha components. The RGB parts are used to encode content and style features. The alpha channel of the \textit{style image} is passed into the network as a soft visibility prior via SoftPConv, modulating feature propagation. In contrast, the \textit{content alpha} is not processed during stylization but preserved for post-processing to maintain downstream compositional fidelity.

\noindent\textbf{Module Replacement Strategy.}
All vanilla convolutions in the encoder of AesFA are replaced with SoftPConv modules, allowing the style alpha map to influence activations throughout the hierarchy. These modified layers serve as drop-in replacements and require no change to the surrounding network topology or loss functions. This design ensures compatibility with pretrained weights and minimizes engineering overhead.

\noindent\textbf{Alpha Preprocessing and Postprocessing.}
Before feature extraction, the style alpha channel is normalized and injected as a soft mask into SoftPConv layers. During decoding, the network reconstructs a stylized RGB image, to which the original content alpha is appended to form the RGBA output. This ensures that style propagation respects stylistic visibility while maintaining structural transparency from the content.

\noindent\textbf{Generalization and Compatibility.}
Although our implementation is instantiated on AesFA, the proposed integration strategy is model-agnostic. Any encoder-decoder-based NST architecture can benefit from alpha-aware modulation by substituting convolutional layers with SoftPConv. This highlights the plug-and-play nature of SoftPConv as a modular enhancement for RGBA-compatible stylization networks.



% 2025-07-24 22:52 周四 版本
% \subsection{Alpha-Aware Network Architecture}

% To demonstrate the general applicability and plug-in nature of Soft Alpha-Guided Feature Propagation (SAFP), we instantiate our method on top of AesFA~\cite{kwon2024aesfa}, a recent state-of-the-art network for arbitrary style transfer. Rather than proposing a new architecture from scratch, we aim to show that transparency-aware stylization can be effectively enabled by augmenting a strong existing baseline with soft visibility priors.

% This design choice facilitates fair comparisons with RGB-only methods while minimizing confounding variables. It also reflects a central goal of our work: to propose a lightweight and modular extension that can be broadly adopted across existing encoder-decoder style transfer pipelines, without disrupting their original structure or performance characteristics.

% In the following, we detail how SoftPConv modules are integrated into AesFA, and how the alpha channel is processed to enable visibility-aware stylization. While our implementation is grounded in AesFA, the methodology is not tied to a specific architecture and can be readily applied to other NST frameworks.


% \noindent\textbf{Input and Output.}
% The model accepts two RGBA images as input: a content image 
% $I_c \in \mathbb{R}^{H \times W \times 4}$  and a style image  $I_s \in \mathbb{R}^{H \times W \times 4}$. The output is a stylized RGBA image $I_t \in \mathbb{R}^{H \times W \times 4}$. The RGB components of both inputs are used for encoding structural and stylistic representations, while the alpha channel of the style image is leveraged during feature extraction to modulate visibility. The content alpha channel, in contrast, is reserved for post-processing to preserve compositional integrity.

% \noindent\textbf{Module Replacement Strategy.}
% To enable transparency-aware feature computation, we replace all vanilla convolution layers in the encoder and decoder of AesFA with SoftPConv modules as described in the previous section of Soft Alpha-Guided Feature Propagation. These modified layers allow the style alpha map to influence intermediate feature activations in a spatially continuous manner. The attention module and style adaptation layers of AesFA are left unchanged, ensuring compatibility with pretrained weights and preserving the model’s expressive power.

% \noindent\textbf{Alpha Preprocessing and Postprocessing.}
% Prior to encoding, the input images are separated into RGB and alpha components. The style alpha map is normalized and injected as a soft visibility mask into the feature propagation pipeline. During decoding, we reconstruct the RGB output via stylization, and append the original content alpha channel as the final alpha output. This design ensures that style is guided by stylistic visibility, while structural transparency from the content image is preserved.

% \noindent\textbf{Generalization and Compatibility.}
% Our framework is designed to be model-agnostic. Since SoftPConv is a drop-in replacement for standard convolution, it can be readily applied to other encoder-decoder style transfer models. We demonstrate this design using AesFA as the base architecture, but the method generalizes to a broad range of NST frameworks.

\subsection{Alpha-Aware Loss Functions}

To ensure that our training objective is consistent with the transparency-aware design of Soft Alpha-Guided Feature Propagation (SAFP), we adapt the loss framework of AesFA~\cite{kwon2024aesfa} by incorporating visibility guidance through the alpha channel. Rather than redesigning all loss terms from scratch, we selectively apply alpha-aware weighting where it is most critical, while keeping the other objectives unchanged to preserve stable baselines.

\noindent\textbf{Content and Style Losses.}
We retain the original content and style reconstruction losses of Gatys et al.~\cite{gatys2016image} without modification. These objectives rely on global feature statistics and are largely agnostic to local visibility variations, making them less sensitive to partial transparency. Preserving their formulation also allows for a cleaner comparison with existing RGB-only baselines.

\noindent\textbf{Alpha-aware EFDM Contrastive Loss.}
The perceptual contrastive loss in AesFA, which is built on Exact Feature Distribution Matching (EFDM)~\cite{zhang2022exact}, is highly sensitive to local feature activations and therefore benefits most from explicit visibility control. We extend EFDM by incorporating alpha masks $M_\alpha$ when matching positive and negative samples: features at transparent or occluded pixels are excluded from the sorted feature distributions before distance computation. 
A subtle challenge arises in this setting: the effective number of valid pixels may differ between the output $O_i$ and the positive sample $S_{\text{pos},i}$ (or negative samples $S_{\text{neg},j}$) due to different alpha masks. Because EFDM relies on comparing sorted feature distributions, unequal sample sizes would result in biased or unstable matching. To address this, we resample the larger set via linear interpolation so that both distributions have the same number of elements before sorting. This step ensures that the alpha-aware EFDM remains a fair and consistent measure of feature alignment.

Formally, the alpha-aware contrastive loss is defined as:
\begin{equation}
    \begin{aligned}
    &\mathcal{L}_{\text{contrastive}}^{\alpha} =\\
    &\sum_{i=1}^N
    \frac{
    \left\|
    F_l(O_i) -
    \text{EFDM}\left(F_l(O_i), F_l(S_{\text{pos}, i}); M_\alpha \right)
    \right\|_2
    }{
    \sum_{j=1}^k
    \left\|
    F_l(O_i) -
    \text{EFDM}\left(F_l(O_i), F_l(S_{\text{neg}, j}); M_\alpha \right)
    \right\|_2
    }
    \label{eq:alpha_efdm}
    \end{aligned}
\end{equation}
where $F_l(\cdot)$ denotes the features extracted at layer $l$, $S_{\text{pos}, i}$ and $S_{\text{neg}, j}$ are the positive and negative style samples, and $\text{EFDM}(\cdot; M_\alpha)$ represents masked feature matching with resampling.

\noindent\textbf{Total Loss.}
Our final training objective is a weighted combination of the three terms:
\begin{equation}
\mathcal{L}_{\text{total}} =
\lambda_c \mathcal{L}_{\text{content}} +
\lambda_s \mathcal{L}_{\text{style}} +
\lambda_{\text{con}} \mathcal{L}_{\text{contrastive}}^{\alpha}
\end{equation}
where $\lambda_c$, $\lambda_s$, and $\lambda_{\text{con}}$ are scalar weights controlling the relative importance of each loss.

\noindent\textbf{Discussion.}
By making the EFDM-based contrastive loss alpha-aware while keeping the content and style losses intact, we achieve two goals: (i) the most visibility-sensitive component of the objective is explicitly aligned with transparency semantics, and (ii) the overall formulation remains plug-and-play and compatible with existing NST frameworks. This minimal yet effective modification closes the loop for SAFP, ensuring that both feature propagation and supervision consistently respect the alpha channel.



\bibliography{aaai2026}

\end{document}
