\subsection{基于模型迭代的风格迁移技术}

基于像素迭代的风格迁移技术虽然取得了开创性的成果，但其中问题依然存在。在考虑将风格迁移方法进行应用时，一个较为明显的缺陷暴露出来，即迁移时间长，无法对进行实时的风格化操作，同时在进行大量特定风格的迁移任务时花费时间巨大，基于模型迭代的风格迁移技术的出现解决了对特定风格进行批量迁移的长耗时问题。按模型与其所能生成的风格数量的对应关系，可以将基于模型迭代的风格迁移技术分为三类：单模型生成单风格、单模型生成多风格以及单模型生成任意风格。

\subsubsection{单模型生成单风格}

Johnson等人\cite{johnsonPerceptualLossesRealTime2016b}与Ulyanov\cite{ulyanovTextureNetworksFeedforward2016b}等人于2016年彼此独立地提出了利用前馈神经网络进行实时风格迁移的想法，其主要思想是训练一个前馈神经网络
\begin{equation}
    \begin{aligned}
        \theta^* &= \arg\min L_{total}\left(I_c,I_s,g_\theta*(I_c)\right),\\
        I^*&=g_\theta*(I_c),
    \end{aligned}
\end{equation}
其中，$\theta^*$为最优的参数，即令损失函数$L_{total}$取最小值的参数；$L_{total}\left(I_c,I_s,g_\theta*(I_c)\right)$是整体的损失函数，它衡量了风格迁移的质量，该函数的输入包括三个部分，$l_c$表示内容图像，$l_s$表示风格图像，$g_\theta*(I_c)$表示对内容图像进行修改从而使其具有与风格图像相似外观的结果，即风格化中的图像；$I^*$是最终生成的图像，也就是风格迁移后所得的风格化图像。该公式的目标是找到一个能够最小化$L_{total}$的参数$\theta^*$，该参数被用于将内容图像$l_c$转换为最终的风格化图像$I^*$。Johnson等人与Ulyanov等人的区别仅在于二者的网络结构的差异：Johnson等人在Rardford等人\cite{radfordUnsupervisedRepresentationLearning2016a}方法的基础上，添加了残差块与分步卷积，并引入的实例归一化层（ Instance Normalization，IN）以加快网络的收敛速度；Ulyanov等人以多尺度结构作为生成网络，目标函数与Gatys等人类似。Johnson等人与Ulyanov等人均以前馈生成网络为基础，达成了实时风格迁移的效果，风格迁移的速度Gatys等人相比，提升了两个数量级。但由于两人的方法遵循了Gatys等人提出方法，所以在迁移效果方面存在类似的问题，如在图像细节与结构合理性方面效果不能令人满意。

利用马尔科夫随机场也可以达成提升迁移速度的目的。Li和Wand等人\cite{liPrecomputedRealTimeTexture2016}在他们之间的工作\cite{liCombiningMarkovRandom2016}的基础上进行改进，他们通过对抗训练获得一个马尔科夫前馈网络，从而解决的效率问题。从原理上来说，该方法类似于他们之前在\cite{liCombiningMarkovRandom2016}中的方法，是一种基于图像块的非参数风格迁移方法，这使得他们的方法在物体结构的合理性方面具有更好的效果，从而更好地保留原图的精细结构。但同样的，该方法遗留了原始方法的一些缺点，如局部块与风格块的匹配度不高时，生成的风格图像效果较差。

利用生成式对抗网络（Generative Adversarial Networks，GAN）进行风格迁移也是一种基于模型迭代的风格迁移方法。GAN由Goodfellow等人\cite{goodfellowGenerativeAdversarialNetworks2020}于2014年提出，该模型采用一个生成网络和一个判别网络进行对抗。在训练时，需要先对判别网络进行训练：给定一组数据，判别网络判断该组数据中的每一个数据项是否属于目标域，并根据真实值与网络计算得到的判断值之间的差距进行优化，直到能够判断测试集中的数据项是否属于目标域：随后对生成器进行训练，生成器生成数据，交由判别器进行判断是否属于目标域，生成器再根据判别器给出的结果进行调整，生成器与判别器二者不断进行对抗，以达到两个网络之间的均衡，从而完成网络的训练，并实现生成数据与真实数据之间分布的相似。GAN的损失函数如下所示：
\begin{equation}
    V(D,G)=\mathbb{E}_{x\sim p_{\text{data}}(x)}\left[\log D(x)\right]+\mathbb{E}_{z\sim p_{\text{z}}(z)}\left[\log \left(1-D(G(z))\right)\right]
\end{equation}
其中，$G$与$D$分别为生成器网络与判别器网络，$p_{\text{data}}(x)$表示数据$x$所满足的分布，$ p_{\text{z}}(z)$表示噪声图像的分布，$D(x)$表示$x$属于真实数据而非输入生成器生成的数据$p_g$的概率，$G(z)$表示生成器对噪声图像的处理结果，$\mathbb E$表示期望。训练判别器的过程以最大化判别器能否识别来自数据集而非生成器$G$的图像的概率；生成器的训练过程以最小化$\log \left(1-D(G(z))\right)$为目标，$1-D(G(z))$表示判别器认为生成器生成的图像不属于真实数据集的概率，最小化该函数及欺骗判别器，使其认为生成器生成的数据来自真实数据集。

GAN模型优秀的网络结构使其能够胜任风格迁移的任务，以下是GoodFellow等人在文献\cite{goodfellowGenerativeAdversarialNets2014a}中给出的一些迁移例子%〔此处需要插入图片：Goodfellow等人的迁移图像〕。

GAN网络同样实现实时风格迁移，但同时存在一些问题与缺陷。首先，GAN的训练通常比较困难，容易出现训练不稳定的情况，如模式崩溃（mode collapse）等。其次，GAN模型的损失函数并未给出类似于Gatys等人\cite{gatysImageStyleTransfer2016}损失函数中的超参数$\alpha$与$\beta$，从而导致了无法控制生成图像与内容图像或风格图像的相似读，在风格迁移任务中难以细粒度地控制两者。

ZHU等人\cite{zhuUnpairedImageToImageTranslation2017a}提出的CycleGAN实现了无监督的风格迁移任务CycleGAN的独特之处在于它引入了“循环一致性损失”，并通过同时训练两个生成器与判别器实现该损失，从而确保了转换是双向的，即可以从一个域到另一个域，然后再返回，在此过程中不会丢失信息。与GAN类似，生成器旨在将图像从一个域映射到另一个域，判别器则试图区分生成的图像和真实图像。CycleGAN 的最大优势之一是其能够处理非配对数据从而实现无监督学习，该方法能够学习如何进行跨域图像转换，而无需在训练时提供每个样本的明确匹配。但同时，CycleGAN在风格迁移领域有一些明显的缺陷，其生成的图像有时可能比真实图像更模糊甚至失真。其次，选择不当的超参数可能导致训练过程不稳定，或者生成质量较差的图像。

\subsubsection{单模型对应多风格}

上述基于模型迭代的风格迁移方法虽然解决了实时风格化的问题，提升了风格迁移的效率，但是每个模型只能对应某个特定的风格，进行新风格的迁移时，需要花费大量的时间进行新模型的训练。利用网络中的参数共享可以很好的实现多风格迁移效果。

Dumoulin等人\cite{dumoulinLearnedRepresentationArtistic2017}等人提出了一种能够生成多种风格的迁移网络。Dumoulin等人认为，部分不同风格的迁移工作中存在相似或者相同的计算部分，因为许多名称不同的艺术风格具有相似或相同的笔触（如印象派绘画具有相似的笔触，区别仅在使用的画面中使用的颜色），将这些具有相似笔触的画作看成不同的风格似乎是很浪费的。而传统一对一的风格迁移模型忽略了这一点，导致在对新风格进行迁移时，造成了不必要的时间浪费。在如何将这个想法付诸实践的过程中，Dumoulin等人\cite{dumoulinLearnedRepresentationArtistic2017}发现，只需要将标准化后的参数进行缩放或者移动，就能适应每种特定的风格；对于一个卷积神经网络而言，这个发现表示网络中所有卷积核的参数可以在不同风格中进行共享。这个发现表明，在进行风格迁移时，只需要在归一化后对某些参数进行调整，即可实现利用对不同风格的迁移。从具体实现层面，Dumoulin等人\cite{dumoulinLearnedRepresentationArtistic2017}在Ulyanov等人\cite{ulyanovTextureNetworksFeedforward2016b}的方法的基础上，在进行实例归一化后继续进行了一次仿射变换，即可完成对不同风格的迁移工作。这个过程被他们乘坐条件归一化（Conditional Instance Normalization，CIN），可以用如下公式表示：
\begin{equation}
    \label{IN}
    IN(x)=\gamma \left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta
\end{equation}
公式中，$\mu(x)$与$\sigma(x)$分别表示图像的均值与标准差。该方法将每种风格与两个参数$\gamma$与$\beta$绑定，从而实现使用单个网络对不同的风格进行迁移的效果。

Chen等人\cite{chenStyleBankExplicitRepresentation2017a}的方法利用了解耦合的思想，即利用单独的网络组件来学习，从而实现了由多个卷积核组成的网络结构“StyleBank”。StyleBank中的每一组卷积核代表了一个特定的风格，将不同组的卷积核放入神经网络中，即可完成对不同风格的迁移工作。同时，由于将实现风格迁移的核心组件——卷积核与其他网络结构拆分开来，所以在对新风格进行训练时，仅需训练对应的卷积核，从而获得了较好的拓展性。

GAN模型依旧能够胜任多风格迁移的任务。Choi\cite{choiStarGANUnifiedGenerative2018a}等人对GAN模型进行改进，同样实现了具有可扩展性与稳定性的、能将图像转换为多个风格的StarGAN。在GAN中生成器仅以图像为输入的基础上，新增了第二个输入——输出图像的风格；同时，判别器在判别时需要给出图像属于某种风格的标签信息。通过这种方式，在仅有一个生成器和一个判别器的情况下，实现了多目标域的风格迁移。

\subsubsection{单模型生成任意风格}

尽管单模型对应多风格的风格迁移已经从一定程度上缓解了模型泛用性低的问题，同时网络结构具有一定的可扩展性，但在向网络中添加新的模型时，依旧需要对新风格进行训练。因此，无需额外训练的任意风格迁移模型成为了研究者们关注的焦点之一。

Chen和Schmidt\cite{chenFastPatchbasedStyle2016a}等人从马尔科夫随机场方法中找到灵感，率先实现了任意风格迁移。他们对CNN输出的特征图进行分块，并匹配最相似的内容特征块与风格特征块，匹配后将其交换，这一匹配并交换的过程被称作“风格交换”。最后通过训练一个神经网络的方式实现从图像块到完整图像的合并。Chen和Schmidt的方法与以往的方法的方法相比，在迁移的灵活性上有了极大的突破，但这是以迁移质量为代价的，Chen和Schmidt的方法风格化的效果通常不如上述其他神经风格迁移方法。这是基于马尔科夫随机场的原理所造成的，正如上文所说的，在内容特征块与风格特征块无法很好匹配时的风格迁移效果不尽人意。

Huang和Serge\cite{huangArbitraryStyleTransfer2017a}提出了另一种实现任意风格迁移的思路，通过在网络中引入一个自适应实例归一化层（Adaptive Instance Normalization，AdaIN），该层用于实现内容特征的方差与均值与风格特征的均值与方差的对应，通过这种方差、均值的匹配实现了任意风格的迁移。Huang和Serge的工作首次完成任意风格的实时迁移，但仅调整图像的方差和均值不足以将图像调整为效果好的风格图像，在合成具有丰富细节、局部结构复杂的风格图时，效果有待提升。

Xu等人\cite{xuDRBGANDynamicResBlock2021}在GAN的基础上进行优化，提出了DRB-GAN模型，使GAN的衍生网络也能做到对任意风格的迁移。他们设计了一个样式集条件鉴别器，该鉴别器在训练阶段使用生成的图像和从目标样式集中采样的几个样式图像作为输入。通过对生成的图像和样式图像的特征进行提取和拼接，然后使用一个小型网络来评估生成图像的质量，以保证特征空间上的风格一致性。该网络在生成具有规则纹理的图案时，效果较好，在测试集上也展现了良好的迁移效果\cite{xuDRBGANDynamicResBlock2021}。


最近一项由Wu和Hu等人提出的研究\cite{wuStyleFormerRealTimeArbitrary2021}将Transformer模型与风格迁移融合，并将其命名为Styleformer，该模型实现了任意风格的迁移。Styleformer可以同时实现细粒度的风格多样性和语义内容一致性。具体来说，Wu和Hu等人\cite{wuStyleFormerRealTimeArbitrary2021} 的Transformer 启发的特征级风格化方法由三个模块组成：1.生成稀疏样式图的生成模块，2.用于全局风格合成的基于transformer的风格合成模块，以及3.用于实现灵活但稳定的风格化的参数化内容调制模块。该方法生成的的风格化图像与内容内容图像在结构一致性较高，对详细的风格变化敏感的同时，仍然在整体上遵循风格图像的风格分布。